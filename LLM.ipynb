{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch\n",
    "import classifier\n",
    "from tokenizer import BertTokenizer\n",
    "from bert import BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removed args from BertDataset in classifier.py\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ele = self.dataset[idx]\n",
    "        return ele\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        sents = [x[0] for x in data]\n",
    "        labels = [x[1] for x in data]\n",
    "        encoding = self.tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "        token_ids = torch.LongTensor(encoding['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "        token_type_ids = torch.LongTensor(encoding['token_type_ids'])\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return token_ids, token_type_ids, attention_mask, labels, sents\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        print(\"collated\")\n",
    "        token_ids, token_type_ids, attention_mask, labels, sents = self.pad_data(all_data)\n",
    "        batched_data = {\n",
    "                'token_ids': token_ids,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels,\n",
    "                'sents': sents,\n",
    "            }\n",
    "\n",
    "        return batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 5 data from reviews.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|██████████| 1/1 [01:50<00:00, 110.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0060, 0.9940],\n",
      "        [0.0108, 0.9892],\n",
      "        [0.1384, 0.8616],\n",
      "        [0.9894, 0.0106],\n",
      "        [0.4132, 0.5868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc :: 0.800\n"
     ]
    }
   ],
   "source": [
    "#Code simplified from test() in classifier.py\n",
    "\n",
    "#Using CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#Load Model from 3.2\n",
    "saved = torch.load(\"flexible-10-1e-05.pt\", map_location=device)\n",
    "config = saved['model_config']\n",
    "model = classifier.BertSentClassifier(config)\n",
    "model.load_state_dict(saved['model'])\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "dev_data = classifier.create_data(\"reviews.txt\", 'test')\n",
    "dev_dataset = BertDataset(dev_data)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=5, collate_fn=dev_dataset.collate_fn)\n",
    "dev_acc, dev_f1, dev_pred, dev_true, dev_sents = classifier.model_eval(dev_dataloader, model, device)\n",
    "\n",
    "with open(\"./reviews_output.txt\", \"w+\") as f:\n",
    "    print(f\"test acc :: {dev_acc :.3f}\")\n",
    "    for s, p in zip(dev_sents, dev_pred):\n",
    "        f.write(f\"{p} ||| {s}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "The model performed ok, it was able to successfully determine the very positive/negative reviews, but struggled with the less negative review. It determined that the random sentence was a positive review. Overall it matches my expectations, the important thing is that it was able to classify the very positive/negative sentences correctly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the seeds\n",
    "seed = 1004804651\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "#Creates an AutoModelForCausalLM from transformers\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "modelLM = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordered by very positive, less positive, less negative, very negative, and random\n",
    "sentences = [\"I really loved this movie! The acting was phenominal and the cinematography was amazing.\",\n",
    "             \"This movie was alright, I just liked the ending.\",\n",
    "             \"The acting could have been better.\",\n",
    "             \"This movie was a waste of my time, I really disliked everything about it.\",\n",
    "             \"Toronto is a city in Ontario, Canada.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenzied_sentences = []\n",
    "for sentence in sentences:\n",
    "    #Appends \"This movie review is\" to each sentence\n",
    "    new_sentence = sentence + \" This polarity of the sentence is\"\n",
    "    \n",
    "    #Tokenzies the sentences\n",
    "    tokenized_sentences.append(tokenizer(new_sentence,return_tensors='pt',padding='max_length',max_length=30,truncation=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  2428,  3866,  2023,  3185,   999,  1996,  3772,  2001,\n",
      "          6887, 16515, 22311,  2140,  1998,  1996, 16434,  2001,  6429,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001, 10303,  1010,  1045,  2074,  4669,  1996,\n",
      "          4566,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 1996, 3772, 2071, 2031, 2042, 2488, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  2023,  3185,  2001,  1037,  5949,  1997,  2026,  2051,  1010,\n",
      "          1045,  2428, 18966,  2673,  2055,  2009,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 4361, 2003, 1037, 2103, 1999, 4561, 1010, 2710, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  1045,  2428,  3866,  2023,  3185,   999,  1996,  3772,  2001,\n",
      "          6887, 16515, 22311,  2140,  1998,  1996, 16434,  2001,  6429,  1012,\n",
      "          2023,  3185,  3319,  2003,   102,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001, 10303,  1010,  1045,  2074,  4669,  1996,\n",
      "          4566,  1012,  2023,  3185,  3319,  2003,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 1996, 3772, 2071, 2031, 2042, 2488, 1012, 2023, 3185, 3319, 2003,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  2023,  3185,  2001,  1037,  5949,  1997,  2026,  2051,  1010,\n",
      "          1045,  2428, 18966,  2673,  2055,  2009,  1012,  2023,  3185,  3319,\n",
      "          2003,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 4361, 2003, 1037, 2103, 1999, 4561, 1010, 2710, 1012, 2023, 3185,\n",
      "         3319, 2003,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  1045,  2428,  3866,  2023,  3185,   999,  1996,  3772,  2001,\n",
      "          6887, 16515, 22311,  2140,  1998,  1996, 16434,  2001,  6429,  1012,\n",
      "          2023,  3185,  3319,  2003,   102,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001, 10303,  1010,  1045,  2074,  4669,  1996,\n",
      "          4566,  1012,  2023,  3185,  3319,  2003,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 1996, 3772, 2071, 2031, 2042, 2488, 1012, 2023, 3185, 3319, 2003,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  2023,  3185,  2001,  1037,  5949,  1997,  2026,  2051,  1010,\n",
      "          1045,  2428, 18966,  2673,  2055,  2009,  1012,  2023,  3185,  3319,\n",
      "          2003,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[ 101, 4361, 2003, 1037, 2103, 1999, 4561, 1010, 2710, 1012, 2023, 3185,\n",
      "         3319, 2003,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[  101,  1045,  2428,  3866,  2023,  3185,   999,  1996,  3772,  2001,\n",
      "          6887, 16515, 22311,  2140,  1998,  1996, 16434,  2001,  6429,  1012,\n",
      "          2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001, 10303,  1010,  1045,  2074,  4669,  1996,\n",
      "          4566,  1012,  2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  1996,  3772,  2071,  2031,  2042,  2488,  1012,  2023, 11508,\n",
      "          3012,  1997,  1996,  6251,  2003,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001,  1037,  5949,  1997,  2026,  2051,  1010,\n",
      "          1045,  2428, 18966,  2673,  2055,  2009,  1012,  2023, 11508,  3012,\n",
      "          1997,  1996,  6251,  2003,   102,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  4361,  2003,  1037,  2103,  1999,  4561,  1010,  2710,  1012,\n",
      "          2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  1045,  2428,  3866,  2023,  3185,   999,  1996,  3772,  2001,\n",
      "          6887, 16515, 22311,  2140,  1998,  1996, 16434,  2001,  6429,  1012,\n",
      "          2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001, 10303,  1010,  1045,  2074,  4669,  1996,\n",
      "          4566,  1012,  2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  1996,  3772,  2071,  2031,  2042,  2488,  1012,  2023, 11508,\n",
      "          3012,  1997,  1996,  6251,  2003,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  2023,  3185,  2001,  1037,  5949,  1997,  2026,  2051,  1010,\n",
      "          1045,  2428, 18966,  2673,  2055,  2009,  1012,  2023, 11508,  3012,\n",
      "          1997,  1996,  6251,  2003,   102,     0,     0,     0,     0,     0]])\n",
      "tensor([[  101,  4361,  2003,  1037,  2103,  1999,  4561,  1010,  2710,  1012,\n",
      "          2023, 11508,  3012,  1997,  1996,  6251,  2003,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "#Creates lists of IDs and attention masks for each review\n",
    "id_list = []\n",
    "attention_mask_list = []\n",
    "for sentence in tokenized_sentences:\n",
    "    ids = torch.LongTensor(sentence['input_ids'])\n",
    "    attention_mask = torch.LongTensor(sentence['attention_mask'])\n",
    "\n",
    "    print(ids)\n",
    "    id_list.append(ids)\n",
    "    attention_mask_list.append(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---=== Sentence 1 ===---\n",
      "tensor(0.2271) yes\n",
      "tensor(0.7729) no\n",
      "---=== Sentence 2 ===---\n",
      "tensor(0.4832) yes\n",
      "tensor(0.5168) no\n",
      "---=== Sentence 3 ===---\n",
      "tensor(0.2220) yes\n",
      "tensor(0.7780) no\n",
      "---=== Sentence 4 ===---\n",
      "tensor(0.3434) yes\n",
      "tensor(0.6566) no\n",
      "---=== Sentence 5 ===---\n",
      "tensor(0.1568) yes\n",
      "tensor(0.8432) no\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "yes_id = tokenizer.convert_tokens_to_ids('yes')\n",
    "no_id = tokenizer.convert_tokens_to_ids('no')\n",
    "for i in range(5):\n",
    "    print(\"---=== Sentence \" + str(i+1) + \" ===---\")\n",
    "    outputs.append(modelLM.forward(input_ids=id_list[i],attention_mask=attention_mask_list[i]))\n",
    "    outputs[i].keys()\n",
    "\n",
    "    #Finds first index where sentence ends\n",
    "    end_index = torch.where(id_list[i][0]==0)[0][0].item()\n",
    "\n",
    "    token_probabilities = F.softmax(outputs[i]['logits'], dim=-1)\n",
    "\n",
    "    #Normalize probabilities of yes/no\n",
    "    yes_prob = token_probabilities[0, end_index, yes_id].detach()\n",
    "    no_prob = token_probabilities[0, end_index, no_id].detach()\n",
    "    print(yes_prob / (yes_prob + no_prob),\"yes\")\n",
    "    print(no_prob / (yes_prob + no_prob),\"no\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "This model performed very poorly compared to the previous one, it believes that all the movie reviews are negative when only 2/4 are explicitly negative. Even modifying the \"This movie review is\" addition to the sentences didn't help either, it even somehow made the random sentence the most polarizing of the bunch, with a probability of 84% being a negative review over a positive one. The previous model was definitely more successful than this model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3\n",
    "\n",
    "Using GPT3's playground, I was able to output the following predictions using a format of \"Predict if the following sentence is positive or negative. + Sentence\". A temperature hyper-parameter setting of 0.7 was optimal as it was able to successfully classify each sentence. The only issue was that the slightly positive sentence was categorized as neutral, but even that can be up to interpretation for humans.\n",
    "\n",
    "![alt text](GPT3.png \"GPT3 Predictions\")\n",
    "\n",
    "## Comments\n",
    "\n",
    "Overall this was the best performing model out of 4.1 and 4.2, as it was also able to determine how neutral the sentence was too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
